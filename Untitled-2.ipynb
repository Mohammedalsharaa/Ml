{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': {'best_params': {'classifier__C': 0.1},\n",
       "  'precision': 0.7878787878787878,\n",
       "  'recall': 0.7027027027027027},\n",
       " 'random_forest': {'best_params': {'classifier__max_depth': 10,\n",
       "   'classifier__n_estimators': 100},\n",
       "  'precision': 0.8181818181818182,\n",
       "  'recall': 0.7297297297297297}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n",
    "train_df.drop(columns=['Cabin'], inplace=True)\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate data\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(columns=['Survived'])\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of numerical and categorical columns\n",
    "num_cols = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "cat_cols = ['Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "# Preprocessing for numerical data: scaling\n",
    "num_transformer = StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data: one-hot encoding\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Define the models and the hyperparameters to tune\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'random_forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a pipeline that combines preprocessing with the model\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Define the scoring metric\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "    }\n",
    "    \n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, scoring=scoring, refit='precision', return_train_score=True)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_pred = grid_search.predict(X_val)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Random Forest model is chosen as the best fit for the Titanic dataset problem due to its superior \n",
    "# performance in precision and recall, its ability to handle complex non-linear relationships, and its \n",
    "# robustness and flexibility in dealing with different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
