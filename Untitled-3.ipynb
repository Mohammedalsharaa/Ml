{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params': {'classifier__max_depth': None,\n",
       "  'classifier__min_samples_leaf': 4,\n",
       "  'classifier__min_samples_split': 2,\n",
       "  'classifier__n_estimators': 100},\n",
       " 'precision': 0.8253968253968254,\n",
       " 'recall': 0.7027027027027027}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n",
    "train_df.drop(columns=['Cabin'], inplace=True)\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate data\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(columns=['Survived'])\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of numerical and categorical columns\n",
    "num_cols = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "cat_cols = ['Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "# Preprocessing for numerical data: scaling\n",
    "num_transformer = StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data: one-hot encoding\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that combines preprocessing with the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score)\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='precision', return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_pred = grid_search.predict(X_val)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "\n",
    "# Display the best parameters and the precision and recall scores\n",
    "results = {\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline:\n",
    "\n",
    "# We create a pipeline that includes preprocessing steps (standardization and one-hot encoding) and the Random Forest classifier.\n",
    "# Hyperparameter Tuning:\n",
    "\n",
    "# We define a grid of hyperparameters to tune: number of estimators, max depth, min samples split, and min samples leaf.\n",
    "# GridSearchCV is used for hyperparameter tuning with 5-fold cross-validation. The model is evaluated using precision and recall metrics.\n",
    "# Model Evaluation:\n",
    "\n",
    "# The best hyperparameters are identified based on the precision score.\n",
    "# We evaluate the final model on the validation set and report precision and recall scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
